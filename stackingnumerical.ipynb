{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9b9f5d",
   "metadata": {},
   "source": [
    "stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac94bc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Accuracy (Numeric): 0.48\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load dataset\n",
    "numeric_df = pd.read_csv(\"numeric_dataset.csv\")\n",
    "np.random.seed(42)\n",
    "numeric_df[\"target\"] = np.random.choice([0, 1], size=len(numeric_df))\n",
    "X = numeric_df.drop(\"target\", axis=1)\n",
    "y = numeric_df[\"target\"]\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# stacking model\n",
    "base_models = [(\"dt\", DecisionTreeClassifier()), (\"rf\", RandomForestClassifier())]\n",
    "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(), passthrough=False)\n",
    "\n",
    "# train\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "print(\"Stacking Accuracy (Numeric):\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c988f76",
   "metadata": {},
   "source": [
    "This Python script demonstrates the use of stacking ensemble learning on a numeric dataset. The code begins by importing essential libraries: pandas and numpy for data handling, and scikit-learn modules for model training, ensemble techniques, and evaluation. The dataset is loaded from \"numeric_dataset.csv\" into a DataFrame called numeric_df. A synthetic binary target column \"target\" is generated using numpy.random.choice to simulate classification labels, with a fixed seed (np.random.seed(42)) to ensure reproducibility.\n",
    "\n",
    "The features (X) and target (y) are separated, and the dataset is split into training and testing sets using an 80:20 ratio via train_test_split. The StackingClassifier is defined with two base models: a DecisionTreeClassifier and a RandomForestClassifier. These base learners capture different patterns and decision boundaries in the data. The predictions from the base models are then used as input features for a meta-model, here LogisticRegression, which learns to optimally combine the base models’ outputs. Setting passthrough=False means only base model predictions—not the original features—are passed to the meta-model.\n",
    "\n",
    "The stacking model is trained on the training data using .fit() and evaluated on the test set with .predict(). Accuracy is calculated via accuracy_score. Stacking leverages the strengths of multiple heterogeneous models, often improving predictive performance compared to single models or homogeneous ensembles like bagging or boosting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
