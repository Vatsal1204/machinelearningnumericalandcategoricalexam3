{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b53ab8a",
   "metadata": {},
   "source": [
    "voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290532ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier on Numeric Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "numeric_df = pd.read_csv(\"numeric_dataset.csv\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "numeric_df[\"target\"] = np.random.choice([0, 1], size=len(numeric_df))\n",
    "\n",
    "X = numeric_df.drop(\"target\", axis=1)\n",
    "y = numeric_df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(\"dt\", dt_clf), (\"rf\", rf_clf)],\n",
    "    voting=\"hard\"\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "print(\"Voting Classifier Accuracy (Numeric Dataset):\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3c499",
   "metadata": {},
   "source": [
    "The program demonstrates the use of a Voting Classifier, an ensemble learning method, on a numeric dataset. The dataset is first loaded from numeric_dataset.csv. Since it doesn’t contain a predefined target variable, a synthetic binary target (0 or 1) is generated randomly to simulate a classification problem.\n",
    "\n",
    "The dataset is then split into features (X) and target (y). To evaluate model performance fairly, the data is divided into training (80%) and testing (20%) subsets using train_test_split.\n",
    "\n",
    "Two base classifiers are chosen:\n",
    "\n",
    "Decision Tree Classifier – a simple, interpretable model that splits data into decision rules.\n",
    "\n",
    "Random Forest Classifier – an ensemble of decision trees that reduces overfitting and improves prediction accuracy.\n",
    "\n",
    "These models are combined using a Voting Classifier, which aggregates the predictions from multiple models. With voting=\"hard\", the final prediction is based on majority voting (the class predicted by most models is selected).\n",
    "\n",
    "The Voting Classifier is trained with fit() on the training set and tested with predict() on the test set. Model performance is evaluated using accuracy_score, which calculates the proportion of correctly predicted labels.\n",
    "\n",
    "By combining models, the Voting Classifier often achieves more stable and accurate results compared to individual classifiers."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
