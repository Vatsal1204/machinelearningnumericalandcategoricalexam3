{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f778a81f",
   "metadata": {},
   "source": [
    "boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b76f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load dataset\n",
    "categorical_df = pd.read_csv(\"categorical_dataset.csv\")\n",
    "\n",
    "# create synthetic target\n",
    "np.random.seed(42)\n",
    "categorical_df[\"target\"] = np.random.choice([0, 1], size=len(categorical_df))\n",
    "\n",
    "X = categorical_df.drop(\"target\", axis=1)\n",
    "y = categorical_df[\"target\"]\n",
    "\n",
    "# automatically detect categorical features\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    [(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)]\n",
    ")\n",
    "\n",
    "# adaboost\n",
    "ada = Pipeline([(\"pre\", preprocessor),\n",
    "                (\"clf\", AdaBoostClassifier(n_estimators=100, random_state=42))])\n",
    "ada.fit(X, y)\n",
    "y_pred = ada.predict(X)\n",
    "print(\"AdaBoost Accuracy (Categorical):\", accuracy_score(y, y_pred))\n",
    "\n",
    "# gradient boosting\n",
    "gb = Pipeline([(\"pre\", preprocessor),\n",
    "               (\"clf\", GradientBoostingClassifier(n_estimators=100, random_state=42))])\n",
    "gb.fit(X, y)\n",
    "y_pred = gb.predict(X)\n",
    "print(\"GradientBoosting Accuracy (Categorical):\", accuracy_score(y, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d157f",
   "metadata": {},
   "source": [
    "This code applies two boosting techniques, AdaBoost and Gradient Boosting, on a categorical dataset. The dataset is read from categorical_dataset.csv, and since it lacks a target column, a synthetic binary target (0 or 1) is generated randomly. Features (X) are separated from the target (y).\n",
    "\n",
    "Because all features are categorical, preprocessing is necessary. The code automatically detects categorical columns and applies OneHotEncoder through a ColumnTransformer, converting categories into binary features that classifiers can use.\n",
    "\n",
    "Two boosting models are then built within Pipelines. The first is AdaBoostClassifier, which trains multiple weak learners (by default, decision stumps) sequentially. Each new learner focuses more on the samples misclassified by the previous ones, thereby reducing bias and improving accuracy. The second model is GradientBoostingClassifier, which also builds models sequentially but optimizes by fitting learners on the residual errors of previous models using gradient descent, making it powerful for complex patterns.\n",
    "\n",
    "Both models are trained (fit) on the dataset, and predictions are made on the same training data. Their accuracies are printed using accuracy_score. Although the results may appear optimistic here (since no train-test split is applied), the script illustrates how boosting methods can handle categorical datasets effectively when combined with proper preprocessing."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
