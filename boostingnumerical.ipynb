{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59870615",
   "metadata": {},
   "source": [
    "boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6740ae4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy (Numeric): 0.5154166666666666\n",
      "GradientBoosting Accuracy (Numeric): 0.50125\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load dataset\n",
    "numeric_df = pd.read_csv(\"numeric_dataset.csv\")\n",
    "np.random.seed(42)\n",
    "numeric_df[\"target\"] = np.random.choice([0, 1], size=len(numeric_df))\n",
    "X = numeric_df.drop(\"target\", axis=1)\n",
    "y = numeric_df[\"target\"]\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# adaboost\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "ada_pred = ada.predict(X_test)\n",
    "print(\"AdaBoost Accuracy (Numeric):\", accuracy_score(y_test, ada_pred))\n",
    "\n",
    "# gradient boosting\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_pred = gb.predict(X_test)\n",
    "print(\"GradientBoosting Accuracy (Numeric):\", accuracy_score(y_test, gb_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61b82b9",
   "metadata": {},
   "source": [
    "The provided Python script demonstrates applying boosting ensemble techniques—specifically AdaBoost and Gradient Boosting—on a numeric dataset. It starts by importing essential libraries: pandas and numpy for data manipulation, and scikit-learn modules for model building, training, and evaluation. The dataset is loaded from \"numeric_dataset.csv\" into a DataFrame called numeric_df. To simulate a target variable, a synthetic binary column \"target\" is created using numpy.random.choice, and a fixed random seed (np.random.seed(42)) ensures reproducibility.\n",
    "\n",
    "Features (X) and target (y) are separated, and the data is split into training and testing sets using an 80:20 ratio via train_test_split. The AdaBoostClassifier is instantiated with 100 estimators (n_estimators=100) and trained on the training set. Predictions on the test set are generated and accuracy is calculated using accuracy_score. Similarly, a GradientBoostingClassifier with 100 estimators is trained and evaluated.\n",
    "\n",
    "Both AdaBoost and Gradient Boosting are sequential ensemble methods that focus on correcting errors of previous models. AdaBoost emphasizes misclassified samples by assigning higher weights, while Gradient Boosting fits new models to the residual errors of previous models. These techniques reduce bias and variance and generally outperform single decision trees. The script prints the test accuracy of both methods, demonstrating the power of boosting for numeric datasets and highlighting the difference between adaptive and gradient-based boosting approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
