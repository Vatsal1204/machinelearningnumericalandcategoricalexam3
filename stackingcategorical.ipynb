{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a17be9",
   "metadata": {},
   "source": [
    "stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1415240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load dataset\n",
    "categorical_df = pd.read_csv(\"categorical_dataset.csv\")\n",
    "np.random.seed(42)\n",
    "categorical_df[\"target\"] = np.random.choice([0, 1], size=len(categorical_df))\n",
    "X = categorical_df.drop(\"target\", axis=1)\n",
    "y = categorical_df[\"target\"]\n",
    "\n",
    "# preprocessing\n",
    "categorical_features = X.columns.tolist()\n",
    "preprocessor = ColumnTransformer([(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)])\n",
    "\n",
    "# pipelines for base models\n",
    "dt_clf = Pipeline([(\"pre\", preprocessor), (\"clf\", DecisionTreeClassifier())])\n",
    "rf_clf = Pipeline([(\"pre\", preprocessor), (\"clf\", RandomForestClassifier())])\n",
    "\n",
    "# stacking model\n",
    "stacking_clf = StackingClassifier(estimators=[(\"dt\", dt_clf), (\"rf\", rf_clf)], final_estimator=LogisticRegression(), passthrough=False)\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "print(\"Stacking Accuracy (Categorical):\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e6f1f",
   "metadata": {},
   "source": [
    "This code demonstrates how to apply stacking ensemble learning to a categorical dataset. The dataset is loaded from categorical_dataset.csv, and since it lacks a target variable, a synthetic binary target (0 or 1) is created randomly. The independent features (X) are separated from the target (y).\n",
    "\n",
    "Because the dataset consists of categorical features, preprocessing is required. A ColumnTransformer is used with OneHotEncoder to convert categorical variables into numerical format suitable for machine learning models.\n",
    "\n",
    "Two base learners are defined: a Decision Tree Classifier and a Random Forest Classifier, each wrapped in a pipeline that includes preprocessing. These models capture different aspects of the dataâ€”decision trees focus on splitting rules, while random forests reduce overfitting by combining multiple trees.\n",
    "\n",
    "A StackingClassifier is then constructed, combining the predictions of these base learners. The final estimator is Logistic Regression, which learns how to best combine the outputs of the decision tree and random forest to make final predictions.\n",
    "\n",
    "The dataset is split into training and testing sets (80/20). The stacking model is trained (fit) and then tested. Finally, accuracy is calculated with accuracy_score, showing how well the stacking ensemble generalizes on unseen data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
